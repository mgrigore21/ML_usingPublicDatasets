{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting land types from satellite imagery\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have two solutions for this problem: \n",
    "    - using Custom Neural Network\n",
    "    - using Pretrained ResNet Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the classification algorithms, I just wanted to check if I have pictures that are completely white or black. At this step, I just wanted to make sure that I do not put into the algorithm completely wrong data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import libraries for pics manipulation,os\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "for folder in os.listdir('images_original'):    \n",
    "    list = os.listdir('images_original/'+folder)\n",
    "    for pic in list:\n",
    "        img=Image.open('images_original/'+folder+'/'+pic)\n",
    "        # convert the pic in black and white and find min/max\n",
    "        extrema = img.convert(\"L\").getextrema()\n",
    "        if extrema == (0, 0):\n",
    "            print('pic is all black'+pic+list)\n",
    "        elif extrema == (1, 1):\n",
    "            print('pic is all white'+pic+list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there were not only white or only black pictures.\n",
    "Next, I will convert all the pictures from (28,28) to (32,32) pixels because ResNet does not take pictures less than (32,32) as an input. This dataset will be used just for Solution with Pretrained ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "\n",
    "files=glob.glob('images_original/*/*.png')\n",
    "for file in files:\n",
    "\n",
    "    im=Image.open(file)\n",
    "    im2=im.resize((32,32),Image.ANTIALIAS)\n",
    "    file=file.replace('\\\\','/')\n",
    "    file=file.replace('images_original','images_resized')\n",
    "    im2.save(file,'PNG',dpi=(100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will construct the classification solution using a pretrained neural network (ResNet).It will just be trained for the demo for several epochs, because of time and processing power constraints. If more training time is available, there is the opportunity to add extra ImageAugmentation methods, try different optimizers, increase epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256803 images belonging to 6 classes.\n",
      "Found 64197 images belonging to 6 classes.\n",
      "Epoch 1/2\n",
      "4012/4012 [==============================] - 923s 230ms/step - loss: 0.1539 - acc: 0.9479 - val_loss: 0.0798 - val_acc: 0.9725\n",
      "Epoch 2/2\n",
      "4012/4012 [==============================] - 927s 231ms/step - loss: 0.0997 - acc: 0.9652 - val_loss: 0.0665 - val_acc: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ffc603be80>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Pretrained neural network (ResNet)\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense,GlobalAveragePooling2D,Flatten,Dropout,Input\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "#from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "pics_xlength=32\n",
    "pics_ylength=32\n",
    "\n",
    "train_data_dir = 'images_resized'\n",
    "epochs=2\n",
    "#batch size is very important, if too small training takes long\n",
    "#if too long, memory issues\n",
    "batch_size=64\n",
    "\n",
    "\n",
    "\n",
    "#import pretrained resnet with 50 layers\n",
    "base_model=ResNet50(weights='imagenet',include_top=False,\n",
    "                 input_shape=(pics_xlength,pics_ylength,3)) \n",
    "#base_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "#add extra two dense layers, one for extra training and\n",
    "#and one for the softmax function\n",
    "model=models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512,activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(6,activation='softmax'))\n",
    "\n",
    "# will retrain last 4 layers in resnet model+ layers added   \n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable=False\n",
    "    \n",
    "#just make sure that training is correct\n",
    "#for layer in base_model.layers:    \n",
    "#    print(layer, layer.trainable)\n",
    "#for layer in model.layers:    \n",
    "#    print(layer, layer.trainable)\n",
    "\n",
    "\n",
    "\n",
    "#Augmentation available\n",
    "#Preprocessing function for resnet50 mandatory\n",
    "'''\n",
    "train_datagen=ImageDataGenerator(\n",
    "                        rotation_range=40,\n",
    "                        width_shift_range=0.2,\n",
    "                        height_shift_range=0.2,\n",
    "                        fill_mode='nearest',\n",
    "                        shear_range=0.2,\n",
    "                        zoom_range=0.2,\n",
    "                        horizontal_flip=True,\n",
    "                        validation_split=0.2,\n",
    "                        preprocessing_function=preprocess_input)\n",
    "'''\n",
    "# Lighter ImageData Generator\n",
    "train_datagen=ImageDataGenerator(\n",
    "                        validation_split=0.2,\n",
    "                        preprocessing_function=preprocess_input)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Input from Directory. Divide in train and validate generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(pics_xlength, pics_ylength),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(pics_xlength, pics_ylength),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples//batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('resnet_2epochs_noaugmentation.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model for a random sample from image dataset. Looks like the algorithm predicts the correct answer with high probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.5269519e-05 9.9987113e-01 4.2752369e-08 3.3256722e-05 3.1623904e-07\n",
      "  1.1554472e-09]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.resnet50 import decode_predictions\n",
    "\n",
    "\n",
    "image = load_img('images_resized/building/3001.png', target_size=(32, 32))\n",
    "image = img_to_array(image)\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "image=preprocess_input(image)\n",
    "pred=model.predict(image)\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the confusion matrix and calculate the balanced accuracy score.\n",
    "The balanced accuracy score takes into consideration the fact that data is not balanced ('Water' class has 120k samples, 'road' class has 8k samples )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 321000 images belonging to 6 classes.\n",
      "Confusion Matrix\n",
      "[[ 71559     31   1628     14    154     11]\n",
      " [   218  11387     28    248      5     37]\n",
      " [  1425      2  47925     11    969     15]\n",
      " [    54    244     19   7865      7      3]\n",
      " [    42      3    587      5  56125     47]\n",
      " [     9     37     39      8     89 120150]]\n",
      "Balanced Accuracy Score\n",
      "0.9714045020677893\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "\n",
    "test_datagen=ImageDataGenerator(\n",
    "                        preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(pics_xlength, pics_ylength),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "Y_pred = model.predict_generator(test_generator, test_generator.samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(test_generator.classes, y_pred))\n",
    "print('Balanced Accuracy Score')\n",
    "print(balanced_accuracy_score(test_generator.classes, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also trained a Custom Convolutional Neural Network with 3 blocks of filters+\n",
    "ReLU+MaxPooling, followed by a Dense layer. Batch Normalization was really important here, it increased accuracy by 5%. I also used a bit more ImageAugmentation tricks than before + Scaling. Size of input pictures is (28,28,3) for this network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256803 images belonging to 6 classes.\n",
      "Found 64197 images belonging to 6 classes.\n",
      "Epoch 1/50\n",
      "4012/4012 [==============================] - 315s 78ms/step - loss: 0.1721 - acc: 0.9390 - val_loss: 0.1419 - val_acc: 0.9467\n",
      "Epoch 2/50\n",
      "4012/4012 [==============================] - 259s 64ms/step - loss: 0.1171 - acc: 0.9599 - val_loss: 0.1071 - val_acc: 0.9624\n",
      "Epoch 3/50\n",
      "4012/4012 [==============================] - 257s 64ms/step - loss: 0.1029 - acc: 0.9649 - val_loss: 0.1451 - val_acc: 0.9481\n",
      "Epoch 4/50\n",
      "4012/4012 [==============================] - 258s 64ms/step - loss: 0.0921 - acc: 0.9692 - val_loss: 0.0745 - val_acc: 0.9739\n",
      "Epoch 5/50\n",
      "4012/4012 [==============================] - 258s 64ms/step - loss: 0.0842 - acc: 0.9719 - val_loss: 0.0698 - val_acc: 0.9753\n",
      "Epoch 6/50\n",
      "4012/4012 [==============================] - 252s 63ms/step - loss: 0.0769 - acc: 0.9748 - val_loss: 0.0618 - val_acc: 0.9796\n",
      "Epoch 7/50\n",
      "4012/4012 [==============================] - 252s 63ms/step - loss: 0.0735 - acc: 0.9759 - val_loss: 0.0703 - val_acc: 0.9742\n",
      "Epoch 8/50\n",
      "4012/4012 [==============================] - 241s 60ms/step - loss: 0.0688 - acc: 0.9771 - val_loss: 0.0684 - val_acc: 0.9758\n",
      "Epoch 9/50\n",
      "4012/4012 [==============================] - 241s 60ms/step - loss: 0.0648 - acc: 0.9786 - val_loss: 0.0789 - val_acc: 0.9726\n",
      "Epoch 10/50\n",
      "4012/4012 [==============================] - 241s 60ms/step - loss: 0.0620 - acc: 0.9798 - val_loss: 0.0519 - val_acc: 0.9824\n",
      "Epoch 11/50\n",
      "4012/4012 [==============================] - 241s 60ms/step - loss: 0.0577 - acc: 0.9813 - val_loss: 0.0640 - val_acc: 0.9792\n",
      "Epoch 12/50\n",
      "4012/4012 [==============================] - 240s 60ms/step - loss: 0.0570 - acc: 0.9815 - val_loss: 0.0556 - val_acc: 0.9814\n",
      "Epoch 13/50\n",
      "4012/4012 [==============================] - 241s 60ms/step - loss: 0.0544 - acc: 0.9825 - val_loss: 0.0397 - val_acc: 0.986444 - acc: \n",
      "Epoch 14/50\n",
      "4012/4012 [==============================] - 240s 60ms/step - loss: 0.0520 - acc: 0.9834 - val_loss: 0.0511 - val_acc: 0.9824\n",
      "Epoch 15/50\n",
      "4012/4012 [==============================] - 241s 60ms/step - loss: 0.0518 - acc: 0.9830 - val_loss: 0.0428 - val_acc: 0.9852\n",
      "Epoch 16/50\n",
      "4012/4012 [==============================] - 241s 60ms/step - loss: 0.0504 - acc: 0.9838 - val_loss: 0.0444 - val_acc: 0.9844\n",
      "Epoch 17/50\n",
      "4012/4012 [==============================] - 242s 60ms/step - loss: 0.0486 - acc: 0.9846 - val_loss: 0.0419 - val_acc: 0.9855\n",
      "Epoch 18/50\n",
      "4012/4012 [==============================] - 242s 60ms/step - loss: 0.0485 - acc: 0.9840 - val_loss: 0.0398 - val_acc: 0.9863\n",
      "Epoch 19/50\n",
      "4012/4012 [==============================] - 242s 60ms/step - loss: 0.0468 - acc: 0.9848 - val_loss: 0.0456 - val_acc: 0.9848\n",
      "Epoch 20/50\n",
      "4012/4012 [==============================] - 242s 60ms/step - loss: 0.0470 - acc: 0.9845 - val_loss: 0.0469 - val_acc: 0.9852\n",
      "Epoch 21/50\n",
      "4012/4012 [==============================] - 242s 60ms/step - loss: 0.0459 - acc: 0.9852 - val_loss: 0.0384 - val_acc: 0.9874\n",
      "Epoch 22/50\n",
      "4012/4012 [==============================] - 242s 60ms/step - loss: 0.0445 - acc: 0.9857 - val_loss: 0.0591 - val_acc: 0.9795\n",
      "Epoch 23/50\n",
      "4012/4012 [==============================] - 270s 67ms/step - loss: 0.0436 - acc: 0.9859 - val_loss: 0.0402 - val_acc: 0.9870\n",
      "Epoch 24/50\n",
      "4012/4012 [==============================] - 242s 60ms/step - loss: 0.0433 - acc: 0.9862 - val_loss: 0.0455 - val_acc: 0.9841\n",
      "Epoch 25/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0426 - acc: 0.9861 - val_loss: 0.0385 - val_acc: 0.9872\n",
      "Epoch 26/50\n",
      "4012/4012 [==============================] - 243s 61ms/step - loss: 0.0422 - acc: 0.9864 - val_loss: 0.0395 - val_acc: 0.9863\n",
      "Epoch 27/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0417 - acc: 0.9864 - val_loss: 0.0428 - val_acc: 0.9847\n",
      "Epoch 28/50\n",
      "4012/4012 [==============================] - 242s 60ms/step - loss: 0.0417 - acc: 0.9862 - val_loss: 0.0434 - val_acc: 0.9851\n",
      "Epoch 29/50\n",
      "4012/4012 [==============================] - 243s 61ms/step - loss: 0.0408 - acc: 0.9866 - val_loss: 0.0382 - val_acc: 0.9872\n",
      "Epoch 30/50\n",
      "4012/4012 [==============================] - 243s 61ms/step - loss: 0.0403 - acc: 0.9869 - val_loss: 0.0352 - val_acc: 0.9877\n",
      "Epoch 31/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0396 - acc: 0.9872 - val_loss: 0.0438 - val_acc: 0.9848\n",
      "Epoch 32/50\n",
      "4012/4012 [==============================] - 243s 61ms/step - loss: 0.0396 - acc: 0.9869 - val_loss: 0.0451 - val_acc: 0.9846\n",
      "Epoch 33/50\n",
      "4012/4012 [==============================] - 243s 61ms/step - loss: 0.0396 - acc: 0.9869 - val_loss: 0.0595 - val_acc: 0.9794\n",
      "Epoch 34/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0383 - acc: 0.9875 - val_loss: 0.0413 - val_acc: 0.9863\n",
      "Epoch 35/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0385 - acc: 0.9875 - val_loss: 0.0389 - val_acc: 0.9867\n",
      "Epoch 36/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0370 - acc: 0.9880 - val_loss: 0.0328 - val_acc: 0.9891\n",
      "Epoch 37/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0373 - acc: 0.9878 - val_loss: 0.0295 - val_acc: 0.9904\n",
      "Epoch 38/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0366 - acc: 0.9880 - val_loss: 0.0401 - val_acc: 0.9857\n",
      "Epoch 39/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0364 - acc: 0.9881 - val_loss: 0.0447 - val_acc: 0.9846\n",
      "Epoch 40/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0362 - acc: 0.9880 - val_loss: 0.0333 - val_acc: 0.9889\n",
      "Epoch 41/50\n",
      "4012/4012 [==============================] - 243s 61ms/step - loss: 0.0361 - acc: 0.9881 - val_loss: 0.0303 - val_acc: 0.9896\n",
      "Epoch 42/50\n",
      "4012/4012 [==============================] - 242s 60ms/step - loss: 0.0365 - acc: 0.9880 - val_loss: 0.0295 - val_acc: 0.9897\n",
      "Epoch 43/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0362 - acc: 0.9881 - val_loss: 0.0315 - val_acc: 0.9891\n",
      "Epoch 44/50\n",
      "4012/4012 [==============================] - 243s 61ms/step - loss: 0.0348 - acc: 0.9888 - val_loss: 0.0441 - val_acc: 0.9853\n",
      "Epoch 45/50\n",
      "4012/4012 [==============================] - 243s 61ms/step - loss: 0.0349 - acc: 0.9888 - val_loss: 0.0359 - val_acc: 0.9876\n",
      "Epoch 46/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0347 - acc: 0.9886 - val_loss: 0.0321 - val_acc: 0.9894\n",
      "Epoch 47/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0345 - acc: 0.9889 - val_loss: 0.0491 - val_acc: 0.9842\n",
      "Epoch 48/50\n",
      "4012/4012 [==============================] - 243s 61ms/step - loss: 0.0345 - acc: 0.9889 - val_loss: 0.0402 - val_acc: 0.9863\n",
      "Epoch 49/50\n",
      "4012/4012 [==============================] - 242s 60ms/step - loss: 0.0341 - acc: 0.9888 - val_loss: 0.0326 - val_acc: 0.9895\n",
      "Epoch 50/50\n",
      "4012/4012 [==============================] - 243s 60ms/step - loss: 0.0333 - acc: 0.9891 - val_loss: 0.0313 - val_acc: 0.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1453b3ddda0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom Convolutional Neural Network\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation,Dropout,Flatten,Dense\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#set pixel length and width\n",
    "pics_xlength=28\n",
    "pics_ylength=28\n",
    "\n",
    "\n",
    "train_data_dir = 'images_original'\n",
    "epochs=50\n",
    "batch_size=64\n",
    "\n",
    "# set input of the neural network\n",
    "# (pics_xlength,pics_ylength,nr_of_channels(rgb))\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, pics_xlength, pics_ylength)\n",
    "else:\n",
    "    input_shape = (pics_xlength, pics_ylength, 3)\n",
    "    \n",
    "\n",
    "# blocks of Conv+Activation+MaxPooling\n",
    "# important params: nr of filters, activation function,pool_size\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "train_datagen=ImageDataGenerator(rescale=1. / 255,                                   \n",
    "                        shear_range=0.2,\n",
    "                        zoom_range=0.2,\n",
    "                        horizontal_flip=True,\n",
    "                        validation_split=0.2)\n",
    "\n",
    "#Input from Directory. Divide in train and validate generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(pics_xlength, pics_ylength),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(pics_xlength, pics_ylength),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = train_generator.samples//batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps = validation_generator.samples // batch_size,\n",
    "    epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
